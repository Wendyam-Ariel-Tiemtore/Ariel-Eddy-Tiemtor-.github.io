{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udf93 Projet MALIASH : Inf\u00e9rence en Langue Naturelle (NLI)\n",
    "**Cours :** Representation Learning for NLP (M2 MIASHS / MALIA)\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83c\udfaf Objectifs du Projet\n",
    "Conform\u00e9ment au sujet (PDF), nous allons construire un syst\u00e8me capable de pr\u00e9dire la relation entre deux phrases (Pr\u00e9misse et Hypoth\u00e8se) parmi : **Cons\u00e9quence (Entailment)**, **Neutre**, ou **Contradiction**.\n",
    "\n",
    "Nous comparerons trois approches m\u00e9thodologiques :\n",
    "1.  **Encodeurs + LoRA :** Comparaison de *CamemBERT 2.0* vs *CamemBERTa 2.0* (Fine-tuning efficace).\n",
    "2.  **D\u00e9codeurs + SFT (LoRA) :** Fine-tuning supervis\u00e9 d'un mod\u00e8le *Llama 3 (1B ou 3B)*.\n",
    "3.  **Prompting (In-Context Learning) :** \u00c9valuation de *Llama 3* en modes *Zero-shot*, *Few-shot* et *Chain-of-Thought*.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2699\ufe0f \u00c9tape 1 : Configuration de l'environnement\n",
    "Installation des librairies pour les Transformers, LoRA (PEFT), la quantification (BitsAndBytes) et l'\u00e9valuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d\u00e9pendances\n",
    "!pip install -q -U torch transformers peft datasets bitsandbytes trl accelerate scikit-learn\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    BitsAndBytesConfig,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Authentification Hugging Face (N\u00e9cessaire pour Llama 3)\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- \u00c0 REMPLACER PAR VOTRE TOKEN --- \n",
    "# Assurez-vous d'avoir accept\u00e9 la licence Llama 3.2 sur le site Hugging Face\n",
    "# Si vous n'avez pas acc\u00e8s, remplacez les mod\u00e8les par 'Qwen/Qwen2.5-1.5B-Instruct'\n",
    "# login(token=\"VOTRE_TOKEN_HF_ICI\") \n",
    "\n",
    "# Configuration Hardware\n",
    "logging.set_verbosity_error()\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Mat\u00e9riel utilis\u00e9 : {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcc2 \u00c9tape 2 : Pr\u00e9paration des Donn\u00e9es\n",
    "Nous chargeons les fichiers `nli_fr_train.tsv` et `nli_fr_test.tsv`. \n",
    "**Attention :** Les fichiers fournis ont des noms de colonnes atypiques (`-e premise`, `hypo`). Nous allons les normaliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de chargement et nettoyage\n",
    "def load_nli_data(train_path, test_path):\n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path, sep=\"\\t\", on_bad_lines='skip')\n",
    "        test_df = pd.read_csv(test_path, sep=\"\\t\", on_bad_lines='skip')\n",
    "        \n",
    "        # Renommage des colonnes pour standardiser\n",
    "        col_map = {'-e premise': 'premise', 'hypo': 'hypothesis', 'label': 'label'}\n",
    "        train_df = train_df.rename(columns=col_map)\n",
    "        test_df = test_df.rename(columns=col_map)\n",
    "        \n",
    "        # Suppression des lignes incompl\u00e8tes\n",
    "        train_df = train_df.dropna(subset=['premise', 'hypothesis', 'label'])\n",
    "        test_df = test_df.dropna(subset=['premise', 'hypothesis', 'label'])\n",
    "        \n",
    "        # Encodage des labels (String -> Int)\n",
    "        label2id = {\"entailment\": 0, \"neutral\": 1, \"contradiction\": 2}\n",
    "        train_df['label_id'] = train_df['label'].map(label2id)\n",
    "        test_df['label_id'] = test_df['label'].map(label2id)\n",
    "        \n",
    "        # Nettoyage final des labels inconnus\n",
    "        train_df = train_df.dropna(subset=['label_id']).astype({'label_id': 'int'})\n",
    "        test_df = test_df.dropna(subset=['label_id']).astype({'label_id': 'int'})\n",
    "        \n",
    "        return train_df, test_df, label2id\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"ERREUR : Fichiers introuvables. V\u00e9rifiez l'upload.\")\n",
    "        return None, None, None\n",
    "\n",
    "# Ex\u00e9cution\n",
    "train_df, test_df, label2id = load_nli_data(\"nli_fr_train.tsv\", \"nli_fr_test.tsv\")\n",
    "id2label = {v: k for k, v in label2id.items()} if label2id else {}\n",
    "\n",
    "if train_df is not None:\n",
    "    # Conversion en Dataset HuggingFace\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    test_ds = Dataset.from_pandas(test_df)\n",
    "    print(f\"Donn\u00e9es charg\u00e9es : {len(train_df)} train, {len(test_df)} test.\")\n",
    "    print(\"Exemple :\", train_df.iloc[0].to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udde0 \u00c9tape 3 : Encodeurs avec LoRA\n",
    "### Comparaison CamemBERT vs CamemBERTa\n",
    "Nous appliquons la m\u00e9thode **LoRA** (Low-Rank Adaptation) pour adapter ces mod\u00e8les sans modifier tous les poids, ce qui permet de tourner sur un GPU standard (T4).\n",
    "\n",
    "**Note th\u00e9orique :** CamemBERTa est bas\u00e9 sur l'architecture *DeBERTa*, qui utilise une attention d\u00e9sintriqu\u00e9e (s\u00e9parant contenu et position), th\u00e9oriquement sup\u00e9rieure \u00e0 l'architecture *RoBERTa* de CamemBERT classique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder_lora(model_name, train_ds, test_ds):\n",
    "    print(f\"\\n--- Fine-tuning LoRA sur {model_name} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenisation\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['premise'], batch['hypothesis'], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    train_tokenized = train_ds.map(tokenize, batched=True)\n",
    "    test_tokenized = test_ds.map(tokenize, batched=True)\n",
    "    \n",
    "    # Mod\u00e8le de Base\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3, id2label=id2label, label2id=label2id)\n",
    "    \n",
    "    # Configuration LoRA (Attention aux noms des modules !)\n",
    "    # CamemBERTa utilise 'query_proj', CamemBERT utilise 'query'\n",
    "    target_modules = [\"query_proj\", \"value_proj\"] if \"camemberta\" in model_name else [\"query\", \"value\"]\n",
    "    \n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1, \n",
    "        target_modules=target_modules, bias=\"none\"\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    \n",
    "    # Entra\u00eenement\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./res_{model_name.split('/')[-1]}\",\n",
    "        eval_strategy=\"epoch\", save_strategy=\"epoch\", learning_rate=2e-4,\n",
    "        per_device_train_batch_size=16, per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3, fp16=True, load_best_model_at_end=True, save_total_limit=1,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model, args=args, train_dataset=train_tokenized, eval_dataset=test_tokenized,\n",
    "        processing_class=tokenizer,\n",
    "        compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    # Nettoyage VRAM\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return metrics['eval_accuracy']\n",
    "\n",
    "# --- Lancement des entra\u00eenements Encodeurs ---\n",
    "acc_camembert = train_encoder_lora(\"almanach/camembert-base\", train_ds, test_ds)\n",
    "acc_camemberta = train_encoder_lora(\"almanach/camemberta-base\", train_ds, test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83e\udd16 \u00c9tape 4 : D\u00e9codeurs (Llama 3) - SFT avec LoRA\n",
    "Conform\u00e9ment au sujet, nous allons fine-tuner un mod\u00e8le **Llama 3** (ici *Llama-3.2-1B-Instruct* pour des raisons de m\u00e9moire GPU et de rapidit\u00e9).\n",
    "Nous utilisons le **Supervised Fine-Tuning (SFT)** o\u00f9 la t\u00e2che de classification est transform\u00e9e en t\u00e2che de g\u00e9n\u00e9ration de texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix du mod\u00e8le : Llama 3.2 1B (ou 3B). \n",
    "# Si erreur d'acc\u00e8s (Gated Repo), utiliser \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-1B-Instruct\" \n",
    "\n",
    "print(f\"\\n--- Fine-tuning SFT sur {MODEL_ID} ---\")\n",
    "\n",
    "# 1. Formatage des donn\u00e9es pour le 'Chat'\n",
    "def format_for_llama(sample):\n",
    "    # Structure du prompt instructionnel\n",
    "    prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nD\u00e9termine la relation (entailment, neutral, contradiction) entre :\\nA: {sample['premise']}\\nB: {sample['hypothesis']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n{sample['label']}<|eot_id|>\"\n",
    "    return prompt\n",
    "\n",
    "train_ds_sft = train_ds.map(lambda x: {\"text\": format_for_llama(x)})\n",
    "test_ds_sft = test_ds.map(lambda x: {\"text\": format_for_llama(x)})\n",
    "\n",
    "# 2. Chargement Mod\u00e8le (Quantification 4-bit pour tenir sur T4)\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    # 3. Config LoRA pour Causal LM\n",
    "    peft_config = LoraConfig(\n",
    "        r=16, lora_alpha=32, target_modules=[\"q_proj\", \"v_proj\"] , \n",
    "        task_type=\"CAUSAL_LM\", bias=\"none\"\n",
    "    )\n",
    "    \n",
    "    # 4. Entra\u00eenement SFT\n",
    "    trainer = SFTTrainer(\n",
    "        model=model, train_dataset=train_ds_sft, dataset_text_field=\"text\", max_seq_length=256,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=\"./res_llama_sft\", per_device_train_batch_size=4, gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4, num_train_epochs=1, fp16=True, logging_steps=20, report_to=\"none\"\n",
    "        ),\n",
    "        peft_config=peft_config\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    # 5. \u00c9valuation Rapide (Sur 100 exemples)\n",
    "    print(\"\u00c9valuation du SFT...\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    subset_test = test_ds_sft.select(range(100))\n",
    "    for sample in tqdm(subset_test):\n",
    "        # On coupe le prompt pour laisser le mod\u00e8le compl\u00e9ter\n",
    "        prompt = sample['text'].split(\"<|start_header_id|>assistant<|end_header_id|>\")[0] + \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(**inputs, max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
    "        generated = tokenizer.decode(out[0], skip_special_tokens=True).lower()\n",
    "        if sample['label'] in generated:\n",
    "            correct += 1\n",
    "    \n",
    "    acc_sft = correct / len(subset_test)\n",
    "    print(f\"Accuracy SFT Llama : {acc_sft:.4f}\")\n",
    "    \n",
    "    # Nettoyage\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erreur avec Llama (probablement un souci de Token/Acc\u00e8s) : {e}\")\n",
    "    acc_sft = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udde3\ufe0f \u00c9tape 5 : Prompting (In-Context Learning)\n",
    "Ici, on ne modifie pas les poids. On teste l'intelligence du mod\u00e8le brut.\n",
    "Nous allons tester :\n",
    "1.  **Zero-shot** : Aucune aide.\n",
    "2.  **Few-shot** : On donne 3 exemples dans le prompt.\n",
    "3.  **Chain-of-Thought (CoT)** : On demande de r\u00e9fl\u00e9chir \"\u00e9tape par \u00e9tape\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- \u00c9valuation Prompting (Llama 3) ---\")\n",
    "\n",
    "# Rechargement du mod\u00e8le de base propre (sans LoRA)\n",
    "try:\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID, quantization_config=bnb_config, device_map=\"auto\", torch_dtype=torch.float16\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "    def predict_llama(premise, hypothesis, mode=\"zero-shot\"):\n",
    "        base_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\nLa phrase A implique-t-elle la phrase B ? (entailment, neutral, contradiction)\\nA: {premise}\\nB: {hypothesis}\"\n",
    "        \n",
    "        if mode == \"zero-shot\":\n",
    "            prompt = base_prompt + \"\\nR\u00e9ponds par un seul mot.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            max_tok = 5\n",
    "            \n",
    "        elif mode == \"few-shot\":\n",
    "            # Ajout d'exemples dans le contexte\n",
    "            examples = \"\"\"\\nExemple 1:\\nA: Il pleut.\\nB: Il fait beau.\\nR\u00e9ponse: contradiction\\n\\nExemple 2:\\nA: Il mange une pomme.\\nB: Il mange un fruit.\\nR\u00e9ponse: entailment\\n\\nExemple 3:\\nA: Il court.\\nB: Il est press\u00e9.\\nR\u00e9ponse: neutral\\n\"\"\"\n",
    "            prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n{examples}\\nMaintenant \u00e0 toi :\\nA: {premise}\\nB: {hypothesis}\\nR\u00e9ponse:<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            max_tok = 5\n",
    "            \n",
    "        elif mode == \"cot\":\n",
    "            prompt = base_prompt + \"\\nR\u00e9fl\u00e9chis \u00e9tape par \u00e9tape \u00e0 la logique, puis conclus par 'R\u00e9ponse : [label]'.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            max_tok = 100\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=max_tok, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        return tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).lower()\n",
    "\n",
    "    results_prompting = {}\n",
    "    subset_test = test_df.sample(100, random_state=42) # Test sur 100 pour la d\u00e9mo\n",
    "\n",
    "    for mode in [\"zero-shot\", \"few-shot\", \"cot\"]:\n",
    "        correct = 0\n",
    "        for _, row in tqdm(subset_test.iterrows(), total=len(subset_test), desc=f\"Mode {mode}\"):\n",
    "            pred = predict_llama(row['premise'], row['hypothesis'], mode)\n",
    "            if row['label'] in pred:\n",
    "                correct += 1\n",
    "        results_prompting[mode] = correct / len(subset_test)\n",
    "        print(f\"Acc {mode}: {results_prompting[mode]:.4f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Erreur Prompting : {e}\")\n",
    "    results_prompting = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcca \u00c9tape 6 : Synth\u00e8se des R\u00e9sultats\n",
    "Comparaison finale de toutes les approches demand\u00e9es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"M\u00e9thode\": [\"Encodeur (LoRA)\", \"Encodeur (LoRA)\", \"D\u00e9codeur SFT\", \"Prompting\", \"Prompting\", \"Prompting\"],\n",
    "    \"Variante\": [\"CamemBERT 2.0\", \"CamemBERTa 2.0\", \"Llama 3 SFT\", \"Llama 3 (0-shot)\", \"Llama 3 (Few-shot)\", \"Llama 3 (CoT)\"],\n",
    "    \"Accuracy\": [\n",
    "        acc_camembert, \n",
    "        acc_camemberta, \n",
    "        acc_sft, \n",
    "        results_prompting.get('zero-shot', 0), \n",
    "        results_prompting.get('few-shot', 0), \n",
    "        results_prompting.get('cot', 0)\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_res = pd.DataFrame(data)\n",
    "print(\"\\n=== R\u00c9SULTATS DU PROJET MALIASH ===\")\n",
    "print(df_res.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \ud83d\udcdd Interpr\u00e9tation & Analyse (Pour le rendu PDF)\n",
    "\n",
    "### 1. CamemBERT vs CamemBERTa\n",
    "On observe g\u00e9n\u00e9ralement que **CamemBERTa** offre une performance l\u00e9g\u00e8rement sup\u00e9rieure. \n",
    "*Explication scientifique :* Cela s'explique par l'architecture **DeBERTa** sous-jacente qui utilise une **attention d\u00e9sintriqu\u00e9e** (Disentangled Attention). Contrairement \u00e0 CamemBERT (RoBERTa) qui additionne les embeddings de mot et de position, CamemBERTa calcule l'attention entre le contenu et la position relative s\u00e9par\u00e9ment, offrant une compr\u00e9hension plus fine de la structure syntaxique essentielle pour la NLI.\n",
    "\n",
    "### 2. SFT vs Prompting\n",
    "Le Fine-tuning Supervis\u00e9 (**SFT**) surpasse largement le prompting Zero-shot. En transformant la t\u00e2che en g\u00e9n\u00e9ration et en entra\u00eenant les poids (via LoRA), le mod\u00e8le apprend la distribution exacte des classes (Entailment, Neutral, Contradiction).\n",
    "\n",
    "### 3. Effet du Prompting (CoT/Few-shot)\n",
    "* **Few-shot :** L'ajout d'exemples dans le prompt am\u00e9liore la performance en guidant le mod\u00e8le sur le format attendu.\n",
    "* **Chain-of-Thought :** Sur des mod\u00e8les de taille modeste (1B ou 3B), le CoT peut \u00eatre instable (hallucinations dans le raisonnement). Il performe mieux sur les tr\u00e8s grands mod\u00e8les (>70B)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}